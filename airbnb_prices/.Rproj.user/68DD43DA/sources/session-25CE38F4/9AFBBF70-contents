---
title: "R Notebook"
output: html_notebook
---

##########
Overfitting
##########

```{r}
library(CodeClanData)
library(modelr)
library(tidyverse)
library(caret)
```

Let's try to build an overfit model 

```{r}
savings
```

```{r}
model_overfit <- lm(savings ~ ., data = savings)

summary(model_overfit)
```

```{r}
model_wellfit <- lm(savings ~ salary + age + retired, 
                    data = savings)

summary(model_wellfit)
```

```{r}
model_underfit <- lm(savings ~ retired, data = savings)

summary(model_underfit)
```


```{r}
broom::glance(model_overfit)
```

r2 - 33.45%
r2' - 30.43%
aic - 25.5k 
bic - 25.8k


```{r}
broom::glance(model_wellfit)
```

r2 - 30.9%
r2' - 30.72%
aic - 25.5k
bic - 25.5k


```{r}
broom::glance(model_wellfit)
```

```{r}
model4 <- lm(savings ~ salary + age + retired + surname, data = savings)

summary(model4)
```

```{r}
anova(model_wellfit, model4)
```

##########
Train-Test split
##########

```{r}
# wouldn't normally do this, but lets all set our random seed to be the same
# number, so that we all get exactly the same results 

set.seed(9)

n_data <- nrow(savings)

# make an index to set aside exactly 20% of our data 
test_index <- sample(1:n_data, size = n_data * 0.2)

# test sample - 20% of data
test <- savings %>% 
  slice(test_index)

# train sample - 80% of data
train <- savings %>% 
  slice(-test_index)
```


```{r}
model <- lm(savings ~ salary + age + retired, data = train)

summary(model)

# model_rmse <- broom::glance(model) %>% 
#   pull(sigma)



train_rmse <- train %>% 
  add_residuals(model) %>% 
  mutate(sq_resid = resid ** 2) %>% 
  summarise(mse = mean(sq_resid),
            rmse = mse ** 0.5) %>% 
  pull(rmse)

train_rmse
```

```{r}
predictions_test <- test %>% 
  add_predictions(model) %>% 
  add_residuals(model) %>% 
  select(savings, pred, resid)

test_rmse <- predictions_test %>% 
  mutate(sq_resid = resid ** 2) %>% 
  summarise(mse = mean(sq_resid),
            rmse = mse ** 0.5) %>% 
  pull(rmse)

test_rmse / train_rmse - 1

predictions_test



  
```


WHATS HAPPENING HERE: 

- train a large sample of data that is a goodfit. Once data is trained we can
then apply it to the much smaller test data 
- we do this to see the impact our model has on data it was not seen before
- for example, if data is overfitted to the train data, it will perform 
terribly on the test data because it had been specifically tailored to the trained 
data 
- a wellfitted model will be able to perform just as well (or close) to on the 
test data as the trained
- gives the model more validity and confidence that it is correct 


K-fold cross validation 

```{r}
# set up options for our "train" function 
cv_10_fold <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = TRUE
)

model <- train(savings ~ salary + age +retired, data = savings,
               trControl = cv_10_fold, 
               method = "lm")
```

```{r}
model$pred %>% 
  group_by(Resample) %>% 
  mutate(sq_resid = (pred - obs)** 2) %>% 
  summarise(mean(sq_resid) ** 0.5)
```

```{r}
model$resample %>% 
  summarise(av_r2 = mean(Rsquared),
            av_rmse = mean(RMSE))
```

```{r}
cv_10_fold
```


```{r}
Find the average error and the average r-squared value across each fold, 
after doing a 10-fold cross validation using the model which has all the 
variables in it. What does this tell you? Are these values as expected?
```


```{r}
model2 <- train(savings ~ salary + ., data = savings,
               trControl = cv_10_fold, 
               method = "lm")
```

```{r}
model2$pred %>% 
  group_by(Resample) %>% 
  mutate(sq_resid = (pred - obs)** 2) %>% 
  summarise(mean(sq_resid) ** 0.5)
```

```{r}
model2$resample %>% 
  summarise(av_r2 = mean(Rsquared),
            av_rmse = mean(RMSE))
```

WHAT IS THIS DOING: 

- comparing the wellfit and overfit models
- how does the train model then apply to the test model i.e. how does this model
fit to data it hasnt seen before. 
- K-fold then says we do this 10 times (or however many specified) 
- at the end we then want to get the avg R^2 and the avg RMSE to see which one is 
better at performing when faced with out of sample data
- avg R^2 if its higher that model explains more of the variance, avg RMSE explains
error between actual values and predictions... want it to be lower. 



```{r}
library(tidyverse)
library(CodeClanData)
library(leaps)
```

```{r}
insurance %>% 
  arrange(age)
```

```{r}
regsubsets_forwards <- regsubsets(charges ~ .,
                                  data = insurance,
                                  nvmax = 8,
                                  method = "forward")

sum_forward <- summary(regsubsets_forwards)
```

```{r}
plot(regsubsets_forwards,
     scale = "adjr2")

plot(regsubsets_forwards,
     scale = "bic")
```

best model at the top ... includes everything except 'sexmale' and 
'regionnorthwest' - for scale = "adjr2"


```{r}
plot(sum_forward$adjr2,
     type = "b")

plot(sum_forward$bic,
     type = "b")
```

Re-run the analyses above using the backward selection and exhaustive search 
variable selection methods [Hint - look at the regsubsets() docs to see how to 
do this]
Compare the tables (or plots, whichever you find easier) showing which 
predictors are included for forward selection, backward selection and exhaustive 
search. Do you find any differences? Use adjusted R-squared as your measure of 
fit.


```{r}
regsubsets_task <- regsubsets(charges ~ .,
                                  data = insurance,
                                  nvmax = 1000,
                                  method = c("forward", "exhaustive", "backward"))

sum_task <- summary(regsubsets_task)

plot(regsubsets_task,
     scale = "adjr2")

plot(regsubsets_task,
     scale = "bic")

plot(sum_task$adjr2,
     type = "b")

plot(sum_task$bic,
     type = "b")

```

```{r}
mod_4vars <- lm(charges ~ age + bmi + smoker + children,
                data = insurance)

summary(mod_4vars)

plot(mod_4vars)
```

```{r}
skimr::skim(insurance) %>% view()
```

