---
title: "R Notebook"
output: html_notebook
---

```{r}
library(car)
library(tidyverse)
library(ggfortify)
library(GGally)
library(modelr)

glimpse(Prestige)
```

```{r}
head(Prestige)
```

```{r}
prestige <- Prestige %>% 
  rownames_to_column("job") 
```

```{r}
prestige %>% 
  select(job, prestige) %>% 
  arrange(desc(prestige))
```

```{r}
prestige %>% 
  filter(is.na(type))

prestige_trim <- prestige %>% 
  mutate(type = coalesce(type, "other")) %>% 
  select(-census)
```

```{r}
skimr::skim(prestige_trim)
```


We observe positive skew in income and prop_women. could be worth log 
transforming?

Now weve eeplored the data, lets explore teh relationships between variables 
(and prestige - our target variable - in particular)

```{r message=FALSE}
prestige_trim %>% 
  select(-job) %>% 
  select(prestige, everything()) %>% 
  ggpairs(aes(colour = type, alpha = 0.5)) +
  theme_classic()
```

```{r}
prestige_trim %>% 
  arrange(desc(prestige))
```


Now we know what's highly correlated, we can try the promising variables
in a model
```{r}
lm1 <- lm(prestige ~ education + income + women + type, data = prestige_trim)

summary(lm1)
```

```{r}
prestige_trim %>% 
  group_by(type) %>% 
  summarise(mean_prestige = mean(prestige)) %>% 
  ggplot(aes(x = type, y = mean_prestige)) + 
  geom_boxplot()
```

intercept is the measure for dummy variable that is omitted to avoid the dummy 
variable trap

e.g. 

results 

intercept - 35 
B1 - 32 == 32 points higher than 35 
B2 - -0.8 == -0.8 lower
B3 - 12 == 12 points higher 


select model with larger R^2 means there is a better fit - explains more of the 
variance of residuals (error term) 


```{r}
autoplot(lm1)
```

we want to build on this and add a second predictor... (I've already done this)

we want to see if there are relationships between the remaining variables and our
model residuals (our prediction errors - standard error)

## need to change my model back 

```{r}
lm2 <- lm(prestige ~ education + income, data = prestige_trim)
```



```{r message=FALSE}
prestige_resid <- prestige_trim %>% 
  add_residuals(lm2) %>% 
  select(-c(prestige, education, job, income))

prestige_resid %>% 
  select(resid, everything()) %>% 
  ggpairs(aes(colour = type, alpha = 0.5))
```



i can add an interactiont term 


```{r}
summary(lm2)

prestige_trim
```

```{r}
# this the same model as I did earlier which included everything 

mod3a <- lm(prestige ~ education + income + type, data = prestige_trim)

summary(mod3a)
```

can we include a categoric where not every level is significant? 

- ANOVA test (*A*nalysis *O*f *VA*riance)

```{r}
# create another model 

mode2a <- lm(prestige ~ education + income, data = prestige_trim)
```


```{r}
anova(mode2a, mod3a)
```

p-value with the categoric variable is ~1.5%, i.e. significant at the 
conventional 5% level --> we can include *all* levels of the type categoric 


```{r}
mode3b <- lm(prestige ~ education + log(income) + type, data = prestige_trim)

summary(mode3b)
```

```{r}
anova(mode2a, mode3b)
```

```{r}
autoplot(mode3b)
```

```{r}
mode4 <- lm(prestige ~ education + log(1 + income) + type + women, 
            data = prestige_trim)

summary(mode4)
```

Interactions: combine the effects of two or more variables 

- good for when your predictor variables aren't independent - i.e.
when there are relationships between them


```{r}
prestige_resid <- prestige_trim %>% 
  add_residuals(mode4)

prestige_resid %>% 
  ggplot(aes(log(1 +), resid, colour = type)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
coplot(resid ~ log(1 + income) | education,
       panel = function(x, y, ...){
         points(x, y)
         abline(lm(y ~ x), col = "blue")
       },
       data = prestige_resid, rows = 1)
```

```{r}
mod5 <- lm(prestige ~ education + log(1 + income) + type + women + 
             education:log(1 + income), data = prestige_trim)

summary(mod5)
autoplot(mod5)
```


# remember notes in multiple regression homework (week 10 , day 2)
